{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RBM_vectors_PCA_clustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IrWLvXP_2jM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "12830834-5765-4a77-e120-b195bd89b485"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsjkV-e2ITBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "fb87386a-1f32-46e6-813c-6c53bbbf1a81"
      },
      "source": [
        "!pip install python_speech_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp36-none-any.whl size=5887 sha256=da79e0eab31f5836e78a08604dfb30092059b3114a1badee14f4c672dfd478ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u24Zyq5FOcHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "dd32af14-bada-4b48-eefb-13eaad65527a"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from __future__ import print_function\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_FkiE1mrS38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from python_speech_features import mfcc\n",
        "from sklearn import preprocessing\n",
        "import scipy.io.wavfile as wav\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6SrhJ0ZxQiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_x_init(fan_in, fan_out, dtype=np.float32):\n",
        "    k =  np.sqrt(6.0 / (fan_in + fan_out))\n",
        "    return tf.random_uniform((fan_in, fan_out), minval=-k, maxval=k, dtype=dtype)\n",
        "\n",
        "\n",
        "def sample_bernoulli(probs):\n",
        "    return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NNcGmW2yOjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class RBM:\n",
        "    def __init__(self,\n",
        "                 n_visible,\n",
        "                 n_hidden,\n",
        "                 learning_rate=0.01,\n",
        "                 momentum=0.95,\n",
        "                 err_function='mse',\n",
        "                 use_tqdm=False,\n",
        "                 # DEPRECATED:\n",
        "                 tqdm=None):\n",
        "        if not 0.0 <= momentum <= 1.0:\n",
        "            raise ValueError('momentum should be in range [0, 1]')\n",
        "\n",
        "       \n",
        "\n",
        "        self._use_tqdm = use_tqdm\n",
        "        self._tqdm = None\n",
        "\n",
        "        if use_tqdm or tqdm is not None:\n",
        "            from tqdm import tqdm\n",
        "            self._tqdm = tqdm\n",
        "\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "\n",
        "        self.x = tf.placeholder(tf.float32, [None, self.n_visible])\n",
        "        self.y = tf.placeholder(tf.float32, [None, self.n_hidden])\n",
        "\n",
        "        self.w = tf.Variable(tf_x_init(self.n_visible, self.n_hidden), dtype=tf.float32)\n",
        "        self.visible_bias = tf.Variable(tf.zeros([self.n_visible]), dtype=tf.float32)\n",
        "        self.hidden_bias = tf.Variable(tf.zeros([self.n_hidden]), dtype=tf.float32)\n",
        "\n",
        "        self.delta_w = tf.Variable(tf.zeros([self.n_visible, self.n_hidden]), dtype=tf.float32)\n",
        "        self.delta_visible_bias = tf.Variable(tf.zeros([self.n_visible]), dtype=tf.float32)\n",
        "        self.delta_hidden_bias = tf.Variable(tf.zeros([self.n_hidden]), dtype=tf.float32)\n",
        "\n",
        "        self.update_weights = None\n",
        "        self.update_deltas = None\n",
        "        self.compute_hidden = None\n",
        "        self.compute_visible = None\n",
        "        self.compute_visible_from_hidden = None\n",
        "\n",
        "        self._initialize_vars()\n",
        "\n",
        "        assert self.update_weights is not None\n",
        "        assert self.update_deltas is not None\n",
        "        assert self.compute_hidden is not None\n",
        "        assert self.compute_visible is not None\n",
        "        assert self.compute_visible_from_hidden is not None\n",
        "\n",
        "        \n",
        "        self.compute_err = tf.reduce_mean(tf.square(self.x - self.compute_visible))\n",
        "\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess = tf.Session()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def _initialize_vars(self):\n",
        "        pass\n",
        "\n",
        "    def get_err(self, batch_x):\n",
        "        return self.sess.run(self.compute_err, feed_dict={self.x: batch_x})\n",
        "\n",
        "\n",
        "\n",
        "    def partial_fit(self, batch_x):\n",
        "        self.sess.run(self.update_weights + self.update_deltas, feed_dict={self.x: batch_x})\n",
        "\n",
        "    def fit(self,\n",
        "            data_x,\n",
        "            n_epoches=10,\n",
        "            batch_size=10,\n",
        "            shuffle=True,\n",
        "            verbose=True):\n",
        "        assert n_epoches > 0\n",
        "\n",
        "        n_data = data_x.shape[0]\n",
        "\n",
        "        if batch_size > 0:\n",
        "            n_batches = n_data // batch_size + (0 if n_data % batch_size == 0 else 1)\n",
        "        else:\n",
        "            n_batches = 1\n",
        "\n",
        "        if shuffle:\n",
        "            data_x_cpy = data_x.copy()\n",
        "            inds = np.arange(n_data)\n",
        "        else:\n",
        "            data_x_cpy = data_x\n",
        "\n",
        "        errs = []\n",
        "\n",
        "        for e in range(n_epoches):\n",
        "            if verbose and not self._use_tqdm:\n",
        "                print('Epoch: {:d}'.format(e))\n",
        "\n",
        "            epoch_errs = np.zeros((n_batches,))\n",
        "            epoch_errs_ptr = 0\n",
        "\n",
        "            if shuffle:\n",
        "                np.random.shuffle(inds)\n",
        "                data_x_cpy = data_x_cpy[inds]\n",
        "\n",
        "            r_batches = range(n_batches)\n",
        "\n",
        "            if verbose and self._use_tqdm:\n",
        "                r_batches = self._tqdm(r_batches, desc='Epoch: {:d}'.format(e), ascii=True, file=sys.stdout)\n",
        "\n",
        "            for b in r_batches:\n",
        "                batch_x = data_x_cpy[b * batch_size:(b + 1) * batch_size]\n",
        "                self.partial_fit(batch_x)\n",
        "                batch_err = self.get_err(batch_x)\n",
        "                epoch_errs[epoch_errs_ptr] = batch_err\n",
        "                epoch_errs_ptr += 1\n",
        "\n",
        "            if verbose:\n",
        "                err_mean = epoch_errs.mean()\n",
        "                if self._use_tqdm:\n",
        "                    self._tqdm.write('Train error: {:.4f}'.format(err_mean))\n",
        "                    self._tqdm.write('')\n",
        "                else:\n",
        "                    print('Train error: {:.4f}'.format(err_mean))\n",
        "                    print('')\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            errs = np.hstack([errs, epoch_errs])\n",
        "\n",
        "        return errs\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.sess.run(self.w),\\\n",
        "            self.sess.run(self.visible_bias),\\\n",
        "            self.sess.run(self.hidden_bias)\n",
        "\n",
        "    def save_weights(self, filename, name):\n",
        "        saver = tf.train.Saver({name + '_w': self.w,\n",
        "                                name + '_v': self.visible_bias,\n",
        "                                name + '_h': self.hidden_bias})\n",
        "        return saver.save(self.sess, filename)\n",
        "\n",
        "    def set_weights(self, w, visible_bias, hidden_bias):\n",
        "        self.sess.run(self.w.assign(w))\n",
        "        self.sess.run(self.visible_bias.assign(visible_bias))\n",
        "        self.sess.run(self.hidden_bias.assign(hidden_bias))\n",
        "\n",
        "    def load_weights(self, filename, name):\n",
        "        saver = tf.train.Saver({name + '_w': self.w,\n",
        "                                name + '_v': self.visible_bias,\n",
        "                                name + '_h': self.hidden_bias})\n",
        "        saver.restore(self.sess, filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2aXzje7yWU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class GBRBM(RBM):\n",
        "    def __init__(self, n_visible, n_hidden, **kwargs):\n",
        "       \n",
        "\n",
        "        RBM.__init__(self, n_visible, n_hidden, **kwargs)\n",
        "\n",
        "    def _initialize_vars(self):\n",
        "        hidden_p = tf.nn.sigmoid(tf.matmul(self.x, self.w) + self.hidden_bias)\n",
        "        visible_recon_p = tf.matmul(sample_bernoulli(hidden_p), tf.transpose(self.w)) + self.visible_bias\n",
        "\n",
        "        \n",
        "\n",
        "        hidden_recon_p = tf.nn.sigmoid(tf.matmul(visible_recon_p, self.w) + self.hidden_bias)\n",
        "\n",
        "        positive_grad = tf.matmul(tf.transpose(self.x), hidden_p)\n",
        "        negative_grad = tf.matmul(tf.transpose(visible_recon_p), hidden_recon_p)\n",
        "\n",
        "        def f(x_old, x_new):\n",
        "            return self.momentum * x_old +\\\n",
        "                   self.learning_rate * x_new * (1 - self.momentum) / tf.to_float(tf.shape(x_new)[0])\n",
        "\n",
        "        delta_w_new = f(self.delta_w, positive_grad - negative_grad)\n",
        "        delta_visible_bias_new = f(self.delta_visible_bias, tf.reduce_mean(self.x - visible_recon_p, 0))\n",
        "        delta_hidden_bias_new = f(self.delta_hidden_bias, tf.reduce_mean(hidden_p - hidden_recon_p, 0))\n",
        "\n",
        "        update_delta_w = self.delta_w.assign(delta_w_new)\n",
        "        update_delta_visible_bias = self.delta_visible_bias.assign(delta_visible_bias_new)\n",
        "        update_delta_hidden_bias = self.delta_hidden_bias.assign(delta_hidden_bias_new)\n",
        "\n",
        "        update_w = self.w.assign(self.w + delta_w_new)\n",
        "        update_visible_bias = self.visible_bias.assign(self.visible_bias + delta_visible_bias_new)\n",
        "        update_hidden_bias = self.hidden_bias.assign(self.hidden_bias + delta_hidden_bias_new)\n",
        "\n",
        "        self.update_deltas = [update_delta_w, update_delta_visible_bias, update_delta_hidden_bias]\n",
        "        self.update_weights = [update_w, update_visible_bias, update_hidden_bias]\n",
        "\n",
        "        self.compute_hidden = tf.nn.sigmoid(tf.matmul(self.x, self.w) + self.hidden_bias)\n",
        "        self.compute_visible = tf.matmul(self.compute_hidden, tf.transpose(self.w)) + self.visible_bias\n",
        "        self.compute_visible_from_hidden = tf.matmul(self.y, tf.transpose(self.w)) + self.visible_bias\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhEkb8RAaX8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data preprocessor\n",
        "# Here Bulk data is preprocessed to use in training\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s_AaIoqUcqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9da12062-17f9-4398-e54f-c4b1d4c6dcf2"
      },
      "source": [
        "shift = 1\n",
        "merge_limit=4\n",
        "dataset = []\n",
        "\n",
        "\n",
        "#name of the file where urbm model learned weights and biases will be saved.\n",
        "save_weights_path = \"/content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_model\" \n",
        "\n",
        "\n",
        "lr  = 0.001\n",
        "input_dim =80\n",
        "hidden_dim = 400\n",
        "batch_size = 100\n",
        "nb_epoch = 100\n",
        "\n",
        "# this is the directory where all test wav data\n",
        "# each file corresponds to each speaker\n",
        "path_dir = \"/content/gdrive/My Drive/SpeakerVerification/Data/\" \n",
        "file_list=os.listdir(path_dir)\n",
        "\n",
        "print(file_list)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['file_example_WAV_2MG.wav', 'taunt.wav', 'StarWars60.wav', 'StarWars3.wav', 'ortalking.wav', '3peopletalking.wav', 'OSR_us_000_0010_8k.wav', 'OSR_us_000_0011_8k.wav', 'OSR_us_000_0012_8k.wav', 'OSR_us_000_0013_8k.wav', 'OSR_us_000_0030_8k.wav', 'OSR_us_000_0031_8k.wav', 'OSR_us_000_0032_8k.wav', 'OSR_us_000_0034_8k.wav', 'OSR_us_000_0035_8k.wav', 'OSR_us_000_0036_8k.wav', '2.wav', '3.wav']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6_ujrlqDX-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "3fac4c43-dedb-47d6-fbd8-127848cb0dc5"
      },
      "source": [
        "# vector \n",
        "import numpy as np\n",
        "\n",
        "supervector_list=[]\n",
        "\n",
        "for f in file_list:\n",
        "    if(f.endswith(\".wav\")==False):\n",
        "        continue\n",
        "\n",
        "    gbrbm = GBRBM(n_visible=input_dim, n_hidden=hidden_dim, learning_rate=lr, momentum=0.95, use_tqdm=True)\n",
        "    gbrbm.load_weights(save_weights_path+f,'weights'+f)\n",
        "    w,b_bias,h_bias = gbrbm.get_weights()\n",
        "\n",
        "    w=np.reshape(w,80*400)\n",
        "    supervector=np.concatenate((w,b_bias,h_bias),axis=None)\n",
        "    # print(supervector.size)\n",
        "    # supervector=preprocessing.scale(supervector) # mean normalization of supervectors\n",
        "    # print(supervector)\n",
        "    supervector_list.append(supervector)\n",
        "\n",
        "\n",
        "# fit current size of the ram\n",
        "trunk_sv_list=[]\n",
        "for item in supervector_list:\n",
        "    new_item=item[:15000]\n",
        "    trunk_sv_list.append(new_item)\n",
        "trunk_vectors= np.array(trunk_sv_list)\n",
        "print(trunk_vectors.shape)\n",
        "#\n",
        "\n",
        "supervector_data=np.array(supervector_list)\n",
        "#normal\n",
        "supervector_data=preprocessing.scale(supervector_data, axis=0)\n",
        "\n",
        "# gbrbm = GBRBM(n_visible=input_dim, n_hidden=hidden_dim, learning_rate=lr, momentum=0.95, use_tqdm=True)\n",
        "# gbrbm.load_weights(save_weights_path,'weights')\n",
        "# w,b_bias,h_bias = gbrbm.get_weights()\n",
        "\n",
        "# w=np.reshape(w,80*400)\n",
        "# urbm_supervector=np.concatenate((w,b_bias,h_bias),axis=None)\n",
        "# print(supervector.size)\n",
        "# u_vector=preprocessing.scale(urbm_supervector) # mean normalization of supervectors\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-107678d0b4b9>:20: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelfile_example_WAV_2MG.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modeltaunt.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelStarWars60.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelStarWars3.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelortalking.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_model3peopletalking.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0010_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0011_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0012_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0013_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0030_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0031_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0032_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0034_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0035_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_modelOSR_us_000_0036_8k.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_model2.wav\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/SpeakerVerification/SavedModels/urbm_model3.wav\n",
            "(18, 15000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:190: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
            "  warnings.warn(\"Numerical issues were encountered \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaqfxI_wwIoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f65182aa-72c5-4646-a6ca-052459a78ab8"
      },
      "source": [
        "# PCA whitening and dimensionality reduction  ...\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "X=supervector_data\n",
        "pca.fit(X)\n",
        "rbm_vectors= pca.transform(X)\n",
        "\n",
        "print(rbm_vectors.shape)\n",
        "print(rbm_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18, 18)\n",
            "[[-7.77966833e+00  1.34871964e+02 -9.05001700e-01  1.85079132e+02\n",
            "  -7.12254944e+01 -3.97905884e+01 -7.54837179e+00  6.68858814e+00\n",
            "   1.14842623e-01  9.61022496e-01 -1.43848801e+00  1.59038270e+00\n",
            "   1.39129257e+00  1.88803887e+00  1.18296874e+00 -3.48188102e-01\n",
            "  -1.30440235e+00 -2.25650147e-05]\n",
            " [-8.55479336e+00  4.34726105e+01  5.48930073e+00  1.53829756e+01\n",
            "   2.39334698e+01  8.35223160e+01  2.92894955e+01 -4.74360695e+01\n",
            "   4.49906206e+00  1.74747410e+01 -8.30813599e+00  8.14800739e+00\n",
            "  -1.36763036e+00  3.30426331e+01  2.43540039e+01 -2.23404522e+01\n",
            "  -2.18531132e+01  7.75735825e-05]\n",
            " [-1.44103813e+01  1.17376587e+02  1.92473557e+02 -1.18840149e+02\n",
            "  -5.08542633e+01 -2.42987137e+01 -4.66350365e+00  5.56161356e+00\n",
            "  -2.38673973e+00  8.49018693e-01  2.00056791e+00  1.63096297e+00\n",
            "   3.46642327e+00  2.24365091e+00  1.79245436e+00 -3.65388244e-02\n",
            "  -3.20069313e+00  6.12121075e-05]\n",
            " [-6.32685041e+00  4.86093521e+01  1.83094845e+01  1.17979765e+01\n",
            "   1.11907864e+01  7.57726212e+01  1.53323183e+01 -2.07410507e+01\n",
            "   1.94025576e+00  1.64439785e+00 -9.64454079e+00 -2.03596258e+00\n",
            "  -1.49783449e+01 -2.80515003e+01 -2.35574627e+01  8.23999500e+00\n",
            "   4.80553589e+01  5.89340925e-05]\n",
            " [ 1.34084625e+02  1.49840760e+00  4.67795706e+00  1.34900675e+01\n",
            "   1.65945473e+01  6.88430481e+01  1.99189072e+01  7.65212402e+01\n",
            "  -2.85358963e+01 -2.21315651e+01  1.81096859e+01 -6.79167318e+00\n",
            "   1.08534794e+01  1.88634467e+00  5.58617592e+00 -9.49355602e-01\n",
            "  -7.10256147e+00  6.12298027e-05]\n",
            " [ 3.48205688e+02 -6.25437622e+01 -2.64355602e+01 -2.27488728e+01\n",
            "  -6.46193161e+01 -2.30913391e+01 -4.22886372e+00 -2.67731934e+01\n",
            "   1.31467152e+01  6.62580109e+00 -4.96704102e+00  1.80323267e+00\n",
            "  -3.92972469e+00  7.50555694e-02 -6.04753137e-01  8.96053910e-02\n",
            "   2.61274600e+00 -1.03258528e-04]\n",
            " [-5.02988472e+01  3.42431259e+01 -8.36957550e+01 -5.00514412e+01\n",
            "  -5.21529055e+00 -2.68119669e+00 -1.43209553e+00  4.40958405e+01\n",
            "   6.42628555e+01  9.93260956e+00 -3.18382282e+01 -1.90101109e+01\n",
            "  -1.64373665e+01  2.05931435e+01 -1.01214828e+01 -2.68307734e+00\n",
            "  -1.85178518e-01  1.98246911e-04]\n",
            " [-4.59592018e+01  3.63840408e+01 -9.84298401e+01 -5.14687195e+01\n",
            "  -3.67358708e+00 -1.40443659e+01 -9.42581749e+00  5.84671259e+00\n",
            "  -2.40472078e+00  3.21755838e+00 -3.20576501e+00  3.05730419e+01\n",
            "   1.19209042e+01 -2.54591408e+01  5.67116203e+01  1.30156469e+01\n",
            "   1.34421015e+01  8.21747817e-05]\n",
            " [-3.84974251e+01  5.19797134e+01 -1.00346870e+02 -5.14141960e+01\n",
            "  -1.02166376e+01 -1.14306030e+01 -2.05232573e+00 -9.48403358e+00\n",
            "  -2.29315853e+01 -3.98485732e+00  1.87268600e+01  2.51526833e+00\n",
            "  -1.99541836e+01 -3.57018051e+01 -2.71700611e+01 -3.47422142e+01\n",
            "  -2.60539799e+01  1.70633197e-04]\n",
            " [-3.89948921e+01  4.12872200e+01 -9.89209442e+01 -4.87850838e+01\n",
            "  -3.14027929e+00 -2.18554363e+01  5.07110023e+00 -2.08916187e+01\n",
            "  -3.81334000e+01 -8.98977375e+00  1.38260822e+01 -1.46294708e+01\n",
            "   2.46369038e+01  4.32673111e+01 -2.04100914e+01  2.66774750e+01\n",
            "   1.12762012e+01  9.06193163e-05]\n",
            " [-6.14411736e+01 -9.84240799e+01  2.06021595e+01  1.25324268e+01\n",
            "  -1.83872089e+01  7.07039118e+00 -2.12750492e+01  1.97899475e+01\n",
            "  -1.19042463e+01  3.83077583e+01 -2.02362347e+01  5.58666496e+01\n",
            "   2.20701141e+01  7.17279482e+00 -2.79575272e+01 -5.46590900e+00\n",
            "  -6.60322011e-01  5.68022951e-05]\n",
            " [-6.21331253e+01 -8.18599319e+01  2.13178425e+01  1.39244690e+01\n",
            "  -1.59815969e+01  2.68289423e+00 -2.91602325e+01  6.42736912e+00\n",
            "  -1.58192987e+01  6.06117973e+01  3.33425941e+01 -3.71387749e+01\n",
            "  -2.71905079e+01 -4.14470226e-01  1.50132332e+01  1.03143015e+01\n",
            "   1.29096651e+00  2.03214586e-06]\n",
            " [-6.70341644e+01 -9.35837173e+01  2.55283623e+01  9.57349396e+00\n",
            "  -2.16348782e+01 -1.07913961e+01 -2.73364773e+01 -3.33631110e+00\n",
            "  -3.39071541e+01 -5.08192482e+01 -4.11762123e+01 -4.25882149e+00\n",
            "  -3.66168976e+01  1.32573891e+01  1.26488781e+01 -8.20982647e+00\n",
            "   4.41396809e+00 -2.18620407e-04]\n",
            " [-6.59969788e+01 -6.14164200e+01  1.85105724e+01  1.43568039e+01\n",
            "  -1.30722361e+01  1.45585527e+01 -1.69646168e+01 -1.14143343e+01\n",
            "   4.64998741e+01 -4.15843124e+01  5.83476830e+01  2.56177788e+01\n",
            "  -8.55608845e+00  7.35076094e+00 -4.47849274e+00  1.27970009e+01\n",
            "  -3.35480905e+00  6.43264502e-06]\n",
            " [-6.41177521e+01 -6.86083984e+01  1.65309048e+01  1.23656054e+01\n",
            "  -2.02306690e+01  1.57668886e+01 -2.45977898e+01 -1.63939724e+01\n",
            "   1.66827469e+01 -1.48929949e+01 -1.27568531e+01 -4.21745949e+01\n",
            "   5.66618309e+01 -2.40291729e+01  1.67841101e+00 -1.24634800e+01\n",
            "  -6.03019857e+00 -5.80671476e-05]\n",
            " [-5.55353012e+01 -7.85952148e+01  2.66391945e+01  1.45082760e+01\n",
            "   6.40597248e+00 -5.44594612e+01  1.21149979e+02  1.06850338e+00\n",
            "   3.00645089e+00  1.48581374e+00 -4.81075466e-01 -2.50734043e+00\n",
            "  -5.09563088e-01 -8.73629856e+00  1.63010728e+00  2.06804419e+00\n",
            "  -3.52510959e-02 -4.97475266e-05]\n",
            " [ 4.37875557e+01  2.17739277e+01  2.68610363e+01  2.05572605e+01\n",
            "   9.92175827e+01 -6.31689548e+00 -1.64669571e+01 -1.11570740e+01\n",
            "   1.28516448e+00 -5.23089990e-02 -2.32822723e+01  6.59831285e-01\n",
            "  -7.09651756e+00 -1.89412670e+01 -9.35940266e+00  4.77150421e+01\n",
            "  -3.27661934e+01 -7.15432689e-05]\n",
            " [ 6.10025978e+01  1.35346260e+01  3.17936668e+01  1.97398872e+01\n",
            "   1.40909134e+02 -5.94567146e+01 -2.56096478e+01  1.62782848e+00\n",
            "   4.58508062e+00  1.34453130e+00  1.29813566e+01  1.41591504e-01\n",
            "   5.63584852e+00  1.05565395e+01  3.06140780e+00 -3.36780663e+01\n",
            "   2.14553547e+01 -3.39562073e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyC-89TS1peE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c9017ccf-a679-4e82-a8ce-a2619276d9f8"
      },
      "source": [
        "# clustering tasks starts here ...\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "X=rbm_vectors\n",
        "clustering = AgglomerativeClustering(n_clusters=6).fit(X)\n",
        "\n",
        "\n",
        "print(clustering.labels_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5 0 4 0 0 3 1 1 1 1 2 2 2 2 2 2 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}